Of course. Here is the code for each file, arranged according to the Low-Level Design (LLD) structure you provided.


configs


configs/global_settings.yaml

YAML



# Global settings for the Lumos Framework Toolkit

# This new structure allows for configuring multiple platforms in the future.
# The framework will look for a key matching the platform name (e.g., 'datahub').
datahub:
  # The DataHub GraphQL Management Service (GMS) server URL.
  gms_server: http://localhost:8080

default_env: DEV
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/configs/global_settings.yaml)


configs/team_configurations/example_team.yaml

YAML



# Example team-specific configuration
team_name: example_team
allowed_platforms:
  - csv
  - mongo
  - avro
default_tags:
  - example
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/configs/team_configurations/example_team.yaml)


core


core/common/__init__.py

Python



# Common utilities and base interfaces for the framework
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/common/__init__.py)


core/common/base_interfaces.py

Python



# Base interface classes for extensibility

class BaseHandler:
    def ingest(self):
        raise NotImplementedError

    def enrich(self):
        raise NotImplementedError
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/common/base_interfaces.py)


core/common/config_manager.py

Python



import yaml
from typing import Any, Dict, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class ConfigManager:
    """
    Manages loading YAML configuration for the framework.
    SRP: Its single responsibility is to load and cache configurations.
    """

    def __init__(self, base_config_dir: str = "configs"):
        self.base_config_dir = Path(base_config_dir)
        self._config_cache: Dict[str, Any] = {}

    def load_config(self, config_path_str: str) -> Dict[str, Any]:
        """
        Load a configuration file by its relative or absolute path.
        Caches the loaded configuration to avoid redundant file I/O.
        """
        config_path = Path(config_path_str)
        absolute_path_str = str(config_path.resolve())

        if absolute_path_str in self._config_cache:
            return self._config_cache[absolute_path_str]

        if not config_path.exists():
            logger.error(f"Configuration file not found at: {config_path_str}")
            return {}

        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
                if not isinstance(config, dict):
                    logger.error(f"Config file {config_path_str} is not a valid dictionary.")
                    return {}
                self._config_cache[absolute_path_str] = config
                logger.info(f"Successfully loaded configuration from {config_path_str}")
                return config
        except yaml.YAMLError as e:
            logger.error(f"Error parsing YAML file {config_path_str}: {e}")
            return {}
        except Exception as e:
            logger.error(f"Error loading config {config_path_str}: {e}")
            return {}

    def get_global_config(self) -> Dict[str, Any]:
        """Get the global framework configuration."""
        global_config_path = self.base_config_dir / "global_settings.yaml"
        return self.load_config(str(global_config_path))
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/common/config_manager.py)


core/common/emitter.py

Python



from source_path.data_catalog_factory import DataCatalogFactory
from configs.global_settings import GLOBAL_SETTINGS

def get_data_catalog():
    """
    Returns a singleton instance of the data catalog.
    """
    config = {"gms_server": GLOBAL_SETTINGS.get("datahub_gms")}
    return DataCatalogFactory.get_instance(platform="datahub", config=config)
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/common/emitter.py)


core/common/urn_builders.py

Python



# URN builder utilities

from datahub.emitter.mce_builder import make_dataset_urn

def build_dataset_urn(platform, name, env):
    return make_dataset_urn(platform=platform, name=name, env=env)
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/common/urn_builders.py)


core/common/utils.py

Python



# General utility functions

import hashlib
import json
import logging
from typing import Any, Dict, List, Optional
from datetime import datetime
import yaml

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def hash_string(s):
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def get_current_timestamp():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def generate_schema_hash(schema: Dict[str, Any]) -> str:
    """Generate a hash for schema metadata."""
    schema_str = json.dumps(schema, sort_keys=True)
    return hashlib.sha256(schema_str.encode()).hexdigest()

def validate_config(config: Dict[str, Any], required_fields: List[str]) -> bool:
    """Validate configuration dictionary has all required fields."""
    return all(field in config for field in required_fields)

def format_timestamp(timestamp: Optional[datetime] = None) -> str:
    """Format timestamp for metadata."""
    if timestamp is None:
        timestamp = datetime.utcnow()
    return timestamp.isoformat()

def merge_metadata(existing: Dict[str, Any], new: Dict[str, Any]) -> Dict[str, Any]:
    """Merge new metadata with existing metadata."""
    merged = existing.copy()
    for key, value in new.items():
        if key in merged and isinstance(merged[key], dict) and isinstance(value, dict):
            merged[key] = merge_metadata(merged[key], value)
        else:
            merged[key] = value
    return merged

def sanitize_entity_id(entity_id: str) -> str:
    """Sanitize entity ID to ensure it's valid."""
    return entity_id.lower().replace(' ', '_').replace('-', '_')

def get_platform_config(platform: str, config_path: str) -> Dict[str, Any]:
    """Load platform-specific configuration."""
    try:
        with open(f"{config_path}/{platform}.yaml", 'r') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logger.error(f"Error loading config for platform {platform}: {str(e)}")
        return {}
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/common/utils.py)


core/platform/__init__.py

Python




# Platform-specific services and interfaces


core/platform/data_catalog_interface.py

Python



from abc import ABC, abstractmethod

class DataCatalog(ABC):
    """
    Abstract base class for a data cataloging platform.
    This defines the contract that any data catalog implementation must follow.
    """

    @abstractmethod
    def emit(self, mce):
        """
        Emits a Metadata Change Event to the data catalog.
        """
        pass

    @abstractmethod
    def emit_mcp(self, mcp):
        """
        Emits a Metadata Change Proposal to the data catalog.
        """
        pass

    @abstractmethod
    def get_emitter(self):
        """
        Returns the underlying emitter object.
        This can be used for platform-specific operations not covered by the interface.
        """
        pass
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/source_path/data_catalog_interface.py)


core/platform/factory.py

Python



from typing import Dict, Any, Optional
from .interface import MetadataPlatformInterface
from .impl.datahub_handler import DataHubHandler

class PlatformFactory:
    """
    Factory for creating metadata platform instances.
    OCP: Can be extended with new platforms without modifying existing code.
    DIP: Provides the correct concrete implementation for the MetadataPlatformInterface abstraction.
    """
    _instances: Dict[str, MetadataPlatformInterface] = {}
    _handler_registry: Dict[str, type] = {
        "datahub": DataHubHandler,
        # To add a new platform, add its handler class here.
        # "amundsen": AmundsenHandler,
    }

    @staticmethod
    def get_instance(platform: str, config: Dict[str, Any]) -> MetadataPlatformInterface:
        """
        Returns a singleton instance of a specific platform handler.
        """
        platform_lower = platform.lower()

        if platform_lower in PlatformFactory._instances:
            return PlatformFactory._instances[platform_lower]

        handler_class = PlatformFactory._handler_registry.get(platform_lower)
        if not handler_class:
            raise ValueError(f"Unsupported data catalog platform: {platform}")

        instance = handler_class(config)
        PlatformFactory._instances[platform_lower] = instance
        return instance
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/source_path/platform_factory.py)


core/platform/interface.py

Python




from abc import ABC, abstractmethod
from typing import Any, Dict, Optional

class MetadataPlatformInterface(ABC):
    """
    Abstract base class for a metadata platform handler.
    ISP: This interface defines the complete contract for platform interactions.
    DIP: High-level services depend on this abstraction, not on concrete implementations.
    """

    @abstractmethod
    def __init__(self, config: Dict[str, Any]):
        """Initialize the platform handler with its specific configuration."""
        self.config = config

    @abstractmethod
    def emit_mce(self, mce: Any) -> None:
        """Emit a Metadata Change Event (MCE)."""
        pass

    @abstractmethod
    def emit_mcp(self, mcp: Any) -> None:
        """Emit a Metadata Change Proposal (MCP)."""
        pass

    @abstractmethod
    def add_lineage(self, upstream_urn: str, downstream_urn: str) -> bool:
        """Add a lineage relationship between two entities."""
        pass
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/source_path/metadata_platform_interface.py)


core/platform/impl/__init__.py

Python



# Concrete implementations of platform handlers


core/platform/impl/datahub_handler.py

Python



import logging
from typing import Any, Dict
from datahub.emitter.rest_emitter import DatahubRestEmitter
from datahub.emitter.mcp import MetadataChangeProposalWrapper
from datahub.metadata.schema_classes import UpstreamClass, UpstreamLineageClass, DatasetLineageTypeClass
from datahub.emitter.mce_builder import make_dataset_urn

from ..interface import MetadataPlatformInterface

logger = logging.getLogger(__name__)

class DataHubHandler(MetadataPlatformInterface):
    """
    DataHub-specific implementation of the MetadataPlatformInterface.
    SRP: Its single responsibility is to handle all communication with a DataHub instance.
    """

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        gms_server = self.config.get("gms_server")
        if not gms_server:
            raise ValueError("DataHub configuration requires 'gms_server'.")
        self._emitter = DatahubRestEmitter(gms_server=gms_server)
        logger.info(f"DataHubHandler initialized for GMS server at {gms_server}")

    def emit_mce(self, mce: Any) -> None:
        """Emits a Metadata Change Event to DataHub."""
        try:
            self._emitter.emit(mce)
            logger.info(f"Successfully emitted MCE to DataHub for URN: {mce.proposedSnapshot.urn}")
        except Exception as e:
            logger.error(f"Failed to emit MCE to DataHub: {e}")
            raise

    def emit_mcp(self, mcp: Any) -> None:
        """Emits a Metadata Change Proposal to DataHub."""
        try:
            self._emitter.emit_mcp(mcp)
            logger.info(f"Successfully emitted MCP to DataHub for URN: {mcp.entityUrn}")
        except Exception as e:
            logger.error(f"Failed to emit MCP to DataHub: {e}")
            raise

    def add_lineage(self, upstream_urn: str, downstream_urn: str) -> bool:
        """Adds dataset lineage to DataHub."""
        try:
            lineage_mcp = MetadataChangeProposalWrapper(
                entityUrn=downstream_urn,
                aspect=UpstreamLineageClass(
                    upstreams=[
                        UpstreamClass(
                            dataset=upstream_urn,
                            type=DatasetLineageTypeClass.TRANSFORMED,
                        )
                    ]
                ),
            )
            self.emit_mcp(lineage_mcp)
            logger.info(f"Successfully added lineage: {upstream_urn} -> {downstream_urn}")
            return True
        except Exception as e:
            logger.error(f"Failed to add lineage to DataHub: {e}")
            return False
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/source_path/datahub_handler.py)


core/platform/impl/datahub_service.py

Python



from .data_catalog_interface import DataCatalog
from datahub.emitter.rest_emitter import DatahubRestEmitter

class DataHubDataCatalog(DataCatalog):
    """
    DataHub-specific implementation of the DataCatalog interface.
    """

    def __init__(self, gms_server):
        self._emitter = DatahubRestEmitter(gms_server=gms_server)

    def emit(self, mce):
        self._emitter.emit(mce)

    def emit_mcp(self, mcp):
        self._emitter.emit_mcp(mcp)

    def get_emitter(self):
        return self._emitter
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/source_path/datahub_service.py)


features


features/__init__.py

Python



# Feature services for the framework


features/dq_services/__init__.py

Python



# Data Quality Assertion Service
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/dq_services/__init__.py)


features/dq_services/assertion_service.py

Python



# Service for DQ assertions

class AssertionService:
    def assert_quality(self, dataset_urn, assertion):
        print(f"Asserting {assertion} on {dataset_urn}")
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/dq_services/assertion_service.py)


features/enrichment/__init__.py

Python



# Enrichment Services: descriptions, tags, docs, properties
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/enrichment_services/__init__.py)


features/enrichment/description_service.py

Python



# Service to add descriptions to datasets or fields

class DescriptionService:
    def add_description(self, urn, description):
        print(f"Adding description to {urn}")
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/enrichment_services/description_service.py)


features/enrichment/documentation_service.py

Python



# Service to attach documentation links

class DocumentationService:
    def add_documentation(self, urn, doc_url):
        print(f"Attaching documentation {doc_url} to {urn}")
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/enrichment_services/documentation_service.py)


features/enrichment/properties_service.py

Python



import logging
from typing import Dict, Any
from source_path.metadata_platform_interface import MetadataPlatformInterface
from datahub.emitter.mcp import MetadataChangeProposalWrapper
from datahub.metadata.schema_classes import DatasetPropertiesClass

logger = logging.getLogger(__name__)

class PropertiesService:
    """Service to manage dataset properties."""

    def __init__(self, platform_handler: MetadataPlatformInterface):
        self.platform_handler = platform_handler

    def update_properties(self, urn: str, config: Dict[str, Any]) -> bool:
        """
        Updates the properties of a dataset.

        Args:
            urn: The URN of the dataset to update.
            config: A dictionary containing the new name, description, and custom properties.
        """
        try:
            properties_aspect = DatasetPropertiesClass(
                name=config.get("name"),
                description=config.get("description"),
                customProperties=config.get("custom_properties")
            )

            mcp = MetadataChangeProposalWrapper(entityUrn=urn, aspect=properties_aspect)
            self.platform_handler.emit_mcp(mcp)

            logger.info(f"Successfully submitted properties update for URN: {urn}")
            return True
        except Exception as e:
            logger.error(f"Failed to update properties for URN {urn}: {e}")
            return False
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/enrichment_services/properties_service.py)


features/enrichment/tag_service.py

Python



# Service to add tags to datasets

class TagService:
    def add_tag(self, urn, tag):
        print(f"Adding tag {tag} to {urn}")
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/enrichment_services/tag_service.py)


features/extraction/__init__.py

Python



# Schema Extraction Service
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/extraction_services/__init__.py)


features/extraction/schema_extractor_service.py

Python



# Service to extract schema from data sources

class SchemaExtractorService:
    def extract(self, source):
        print(f"Extracting schema from {source}")
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/extraction_services/schema_extractor_service.py)


features/ingestion/__init__.py

Python



# Ingestion services and handlers


features/ingestion/service.py

Python



import logging
from typing import Dict, Any
from pathlib import Path
from .handlers.base import BaseIngestionHandler
from .handlers.csv import CSVIngestionHandler
from .handlers.mongo import MongoIngestionHandler
from .handlers.avro import AvroIngestionHandler
from core.common.config_manager import ConfigManager
from core.platform.interface import MetadataPlatformInterface

logger = logging.getLogger(__name__)

class IngestionService:
    """
    Manages and orchestrates the data ingestion process across different sources.
    SRP: Its single responsibility is to manage the ingestion lifecycle.
    OCP: Can be extended with new handlers without modifying the service's own code.
    """

    def __init__(self, config_manager: ConfigManager, platform_handler: MetadataPlatformInterface):
        self.config_manager = config_manager
        self.platform_handler = platform_handler
        self._handler_registry: Dict[str, type[BaseIngestionHandler]] = {
            "csv": CSVIngestionHandler,
            "mongodb": MongoIngestionHandler,
            "avro": AvroIngestionHandler,
            # To add support for a new source, register its handler here.
            # "parquet": ParquetIngestionHandler,
        }

    def _get_handler(self, config: Dict[str, Any]) -> BaseIngestionHandler:
        """Get the appropriate ingestion handler for a given configuration."""
        source_type = config.get("source", {}).get("type")
        if not source_type:
            raise ValueError("Source 'type' not specified in configuration.")

        handler_class = self._handler_registry.get(source_type.lower())
        if not handler_class:
            raise NotImplementedError(f"No handler found for source type: {source_type}")

        return handler_class(config, self.platform_handler)

    def start_ingestion(self, config_path: str) -> bool:
        """
        Starts the ingestion process based on a given configuration file.
        """
        try:
            config = self.config_manager.load_config(config_path)
            if not config:
                raise ValueError(f"Could not load or parse config from {config_path}")

            handler = self._get_handler(config)

            logger.info(f"Starting ingestion using handler: {handler.__class__.__name__}")
            handler.ingest()

            return True

        except Exception as e:
            logger.error(f"Fatal error during ingestion process for {config_path}: {e}", exc_info=True)
            return False
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/ingestion_handlers/ingestion_service.py)


features/ingestion/handlers/__init__.py

Python



# Ingestion handlers for various data sources


features/ingestion/handlers/avro.py

Python



import os
import logging
from typing import Dict, Any
from fastavro import reader
from .base import BaseIngestionHandler
from core.platform.interface import MetadataPlatformInterface
from datahub.metadata.schema_classes import (
    MetadataChangeEventClass, DatasetSnapshotClass, SchemaMetadataClass,
    SchemaFieldClass, SchemaFieldDataTypeClass, StringTypeClass, NumberTypeClass,
    BooleanTypeClass, OtherSchemaClass, DatasetPropertiesClass
)
from datahub.emitter.mce_builder import make_dataset_urn

logger = logging.getLogger(__name__)

class AvroIngestionHandler(BaseIngestionHandler):
    """Handler for Avro file ingestion."""

    def __init__(self, config: Dict[str, Any], platform_handler: MetadataPlatformInterface):
        super().__init__(config, platform_handler)
        self.required_fields.extend(["directory_path"])

    def ingest(self) -> None:
        """Ingests metadata from all Avro files in a specified directory."""
        if not self.validate_config():
            raise ValueError("Avro source config validation failed.")

        avro_dir = self.source_config["directory_path"]
        env = self.sink_config.get("env", "PROD")
        platform = "avro"

        if not os.path.isdir(avro_dir):
            logger.error(f"Provided path is not a directory: {avro_dir}")
            raise FileNotFoundError(f"Directory not found: {avro_dir}")

        avro_files = [f for f in os.listdir(avro_dir) if f.endswith(".avro")]
        if not avro_files:
            logger.warning(f"No Avro files found in directory: {avro_dir}")
            return

        logger.info(f"Found {len(avro_files)} Avro files to process in {avro_dir}.")
        for avro_file in avro_files:
            self._ingest_file(os.path.join(avro_dir, avro_file), platform, env)

    def _ingest_file(self, file_path: str, platform: str, env: str):
        """Processes a single Avro file."""
        dataset_name = os.path.splitext(os.path.basename(file_path))[0]
        logger.info(f"Processing Avro file: {dataset_name}")

        try:
            with open(file_path, "rb") as fo:
                avro_reader = reader(fo)
                avro_schema = avro_reader.writer_schema

                # 1. Create Schema Fields
                schema_fields = []
                type_mapping = {
                    "string": StringTypeClass(), "int": NumberTypeClass(), "long": NumberTypeClass(),
                    "float": NumberTypeClass(), "double": NumberTypeClass(), "boolean": BooleanTypeClass()
                }
                for field in avro_schema.get("fields", []):
                    dtype = field["type"]
                    if isinstance(dtype, list): # Handle nullable fields like ["null", "string"]
                        dtype = next((t for t in dtype if t != "null"), "string")

                    schema_fields.append(SchemaFieldClass(
                        fieldPath=field["name"],
                        nativeDataType=str(field["type"]),
                        type=SchemaFieldDataTypeClass(type=type_mapping.get(dtype, StringTypeClass()))
                    ))

                # 2. Create Aspects
                dataset_urn = make_dataset_urn(platform, dataset_name, env)
                schema_metadata = SchemaMetadataClass(
                    schemaName=dataset_name, platform=f"urn:li:dataPlatform:{platform}", version=0, hash="",
                    platformSchema=OtherSchemaClass(rawSchema=str(avro_schema)), fields=schema_fields
                )
                dataset_properties = DatasetPropertiesClass(name=dataset_name)

                # 3. Create and emit MCE
                snapshot = DatasetSnapshotClass(urn=dataset_urn, aspects=[schema_metadata, dataset_properties])
                mce = MetadataChangeEventClass(proposedSnapshot=snapshot)
                self.platform_handler.emit_mce(mce)

        except Exception as e:
            logger.error(f"Failed to process Avro file {file_path}: {e}", exc_info=True)
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/ingestion_handlers/avro_handler.py)


features/ingestion/handlers/base.py

Python



from abc import ABC, abstractmethod
from typing import Any, Dict
import logging
from core.platform.interface import MetadataPlatformInterface

logger = logging.getLogger(__name__)

class BaseIngestionHandler(ABC):
    """
    Abstract Base Class for all ingestion handlers.
    LSP: Any subclass of BaseIngestionHandler can be used by IngestionService without issue.
    OCP: The system is extended by creating new subclasses of this handler.
    """

    def __init__(self, config: Dict[str, Any], platform_handler: MetadataPlatformInterface):
        self.source_config = config.get("source", {})
        self.sink_config = config.get("sink", {})
        self.platform_handler = platform_handler
        self.required_fields = ["type"]

    def validate_config(self) -> bool:
        """Validate the handler's source configuration."""
        is_valid = all(field in self.source_config for field in self.required_fields)
        if not is_valid:
            logger.error(f"Invalid config for handler. Missing required fields: {self.required_fields}")
        return is_valid

    @abstractmethod
    def ingest(self) -> None:
        """
        Main ingestion method.
        This method should orchestrate the extraction, transformation, and emission of metadata.
        """
        pass
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/ingestion_handlers/base_ingestion_handler.py)


features/ingestion/handlers/csv.py

Python



import pandas as pd
import logging
from typing import Dict, Any
from .base import BaseIngestionHandler
from core.platform.interface import MetadataPlatformInterface
from datahub.metadata.schema_classes import (
    MetadataChangeEventClass, DatasetSnapshotClass, SchemaMetadataClass,
    SchemaFieldClass, SchemaFieldDataTypeClass, StringTypeClass, NumberTypeClass,
    OtherSchemaClass, DatasetPropertiesClass
)
from datahub.emitter.mce_builder import make_dataset_urn

logger = logging.getLogger(__name__)

class CSVIngestionHandler(BaseIngestionHandler):
    """Handler for CSV file ingestion."""

    def __init__(self, config: Dict[str, Any], platform_handler: MetadataPlatformInterface):
        super().__init__(config, platform_handler)
        self.required_fields.extend(["path", "dataset_name"])

    def ingest(self) -> None:
        """Ingests metadata from a CSV file to the metadata platform."""
        if not self.validate_config():
            raise ValueError("CSV source config validation failed.")

        file_path = self.source_config["path"]
        dataset_name = self.source_config["dataset_name"]
        env = self.sink_config.get("env", "PROD")
        platform = "csv" # The platform of the source data itself

        logger.info(f"Reading CSV from {file_path} to generate metadata for dataset '{dataset_name}'.")

        try:
            df = pd.read_csv(file_path, delimiter=self.source_config.get("delimiter", ","))
        except FileNotFoundError:
            logger.error(f"CSV file not found at {file_path}")
            raise

        # 1. Create Schema Fields from DataFrame
        type_mapping = {"int64": NumberTypeClass(), "float64": NumberTypeClass()}
        schema_fields = []
        for col_name, dtype in df.dtypes.items():
            field = SchemaFieldClass(
                fieldPath=col_name,
                nativeDataType=str(dtype),
                type=SchemaFieldDataTypeClass(type=type_mapping.get(str(dtype), StringTypeClass()))
            )
            schema_fields.append(field)

        # 2. Create SchemaMetadata aspect
        schema_metadata = SchemaMetadataClass(
            schemaName=dataset_name,
            platform=f"urn:li:dataPlatform:{platform}",
            version=0,
            hash="",
            platformSchema=OtherSchemaClass(rawSchema=""),
            fields=schema_fields
        )

        # 3. Create DatasetProperties aspect
        dataset_properties = DatasetPropertiesClass(
            name=dataset_name,
            description=f"Dataset ingested from CSV file: {file_path}"
        )

        # 4. Create DatasetSnapshot
        dataset_urn = make_dataset_urn(platform, dataset_name, env)
        snapshot = DatasetSnapshotClass(
            urn=dataset_urn,
            aspects=[schema_metadata, dataset_properties]
        )

        # 5. Create MCE and emit
        mce = MetadataChangeEventClass(proposedSnapshot=snapshot)
        self.platform_handler.emit_mce(mce)

        logger.info(f"Successfully ingested metadata for CSV dataset: {dataset_name}")
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/ingestion_handlers/csv_handler.py)


features/ingestion/handlers/factory.py

Python



# Import the concrete handlers you want the factory to know about
from .csv import CSVIngestionHandler
from .mongo import MongoIngestionHandler
from .avro import AvroIngestionHandler
# from .parquet import ParquetIngestionHandler # Example for the future

class HandlerFactory:
    """
    Factory to create the appropriate ingestion handler based on configuration.
    """
    @staticmethod
    def get_handler(config: dict):
        """
        Takes an ingestion configuration and returns the correct handler instance.
        """
        source_type = config.get("source", {}).get("type")

        if source_type == "csv":
            # The CSVIngestionHandler expects the full pipeline config
            return CSVIngestionHandler(config)
        elif source_type == "mongo":
            return MongoIngestionHandler()
        elif source_type == "avro":
            return AvroIngestionHandler()
        # To add a new handler, you would just add an elif block here
        # elif source_type == "parquet":
        #   return ParquetIngestionHandler(config)
        else:
            raise ValueError(f"No handler found for source type: {source_type}")
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/ingestion_handlers/handler_factory.py)


features/ingestion/handlers/mongo.py

Python



import pymongo
import logging
from typing import Dict, Any, List
from .base import BaseIngestionHandler
from core.platform.interface import MetadataPlatformInterface
from datahub.metadata.schema_classes import (
    MetadataChangeEventClass, DatasetSnapshotClass, SchemaMetadataClass,
    SchemaFieldClass, SchemaFieldDataTypeClass, StringTypeClass, NumberTypeClass,
    BooleanTypeClass, TimeTypeClass, OtherSchemaClass, DatasetPropertiesClass
)
from datahub.emitter.mce_builder import make_dataset_urn

logger = logging.getLogger(__name__)

class MongoIngestionHandler(BaseIngestionHandler):
    """Handler for MongoDB ingestion."""

    def __init__(self, config: Dict[str, Any], platform_handler: MetadataPlatformInterface):
        super().__init__(config, platform_handler)
        self.required_fields.extend(["uri", "database"])

    def _map_python_type_to_datahub_type(self, py_type: str) -> SchemaFieldDataTypeClass:
        type_mapping = {
            'str': StringTypeClass(), 'int': NumberTypeClass(), 'float': NumberTypeClass(),
            'bool': BooleanTypeClass(), 'datetime': TimeTypeClass(),
        }
        return SchemaFieldDataTypeClass(type=type_mapping.get(py_type, StringTypeClass()))

    def ingest(self) -> None:
        """Ingests metadata from a MongoDB database to the metadata platform."""
        if not self.validate_config():
            raise ValueError("MongoDB source config validation failed.")

        uri = self.source_config["uri"]
        database_name = self.source_config["database"]
        env = self.sink_config.get("env", "PROD")
        platform = "mongodb"

        try:
            client = pymongo.MongoClient(uri, serverSelectionTimeoutMS=5000)
            client.server_info() # Validate connection
            db = client[database_name]
            logger.info(f"Connected to MongoDB database: {database_name}")

            collections = self.source_config.get("collections", db.list_collection_names())
            logger.info(f"Found {len(collections)} collections to process: {collections}")

            for coll_name in collections:
                self._ingest_collection(db, coll_name, platform, env)

        except pymongo.errors.ServerSelectionTimeoutError as e:
            logger.error(f"Could not connect to MongoDB at {uri}: {e}")
            raise
        finally:
            if 'client' in locals():
                client.close()

    def _ingest_collection(self, db: Any, coll_name: str, platform: str, env: str):
        logger.info(f"Processing collection: {coll_name}")
        collection = db[coll_name]
        sample = collection.find_one()

        if not sample:
            logger.warning(f"No documents found in collection '{coll_name}'. Skipping.")
            return

        field_info = {field: type(value).__name__ for field, value in sample.items()}
        schema_fields = [
            SchemaFieldClass(
                fieldPath=field,
                nativeDataType=py_type,
                type=self._map_python_type_to_datahub_type(py_type)
            ) for field, py_type in field_info.items()
        ]

        dataset_name = f"{db.name}.{coll_name}"
        dataset_urn = make_dataset_urn(platform, dataset_name, env)

        schema_metadata = SchemaMetadataClass(
            schemaName=coll_name,
            platform=f"urn:li:dataPlatform:{platform}",
            version=0, hash="",
            platformSchema=OtherSchemaClass(rawSchema=""),
            fields=schema_fields
        )
        dataset_properties = DatasetPropertiesClass(name=coll_name)
        snapshot = DatasetSnapshotClass(urn=dataset_urn, aspects=[schema_metadata, dataset_properties])
        mce = MetadataChangeEventClass(proposedSnapshot=snapshot)

        self.platform_handler.emit_mce(mce)
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/ingestion_handlers/mongo_handler.py)


features/ingestion/handlers/parquet.py

Python



# Parquet Ingestion Handler

class ParquetIngestionHandler:
    def ingest(self, file_path, config):
        print(f"Ingesting Parquet file: {file_path}")
        # Actual logic to read Parquet and emit to DataHub
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/ingestion_handlers/parquet_handler.py)


features/ingestion/handlers/s3.py

Python



# S3 Ingestion Handler

class S3IngestionHandler:
    def ingest(self, bucket, key, config):
        print(f"Ingesting from S3: s3://{bucket}/{key}")
        # Logic for S3 ingestion and DataHub emission
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/ingestion_handlers/s3_handler.py)


features/lineage/__init__.py

Python



# Lineage Service: dataset and job lineage logic
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/lineage_services/__init__.py)


features/lineage/data_job_service.py

Python



from datahub.metadata.schema_classes import (
    UpstreamLineageClass, UpstreamClass, DatasetLineageTypeClass,
    DataJobInputOutputClass, DataJobInfoClass
)
from datahub.emitter.mcp import MetadataChangeProposalWrapper
from datahub.emitter.mce_builder import make_dataset_urn, make_data_job_urn
from core.common.emitter import get_data_catalog

def update_lineage_and_job():
    data_catalog = get_data_catalog()

    # Define lineage
    upstream = UpstreamClass(
        dataset=make_dataset_urn("csv", "location_master_dimension", env="DEV"),
        type=DatasetLineageTypeClass.TRANSFORMED
    )
    lineage = UpstreamLineageClass(upstreams=[upstream])

    # Define dataset URN
    dataset_urn = make_dataset_urn("csv", "node_master_dimension", env="DEV")

    # Create MCP for lineage
    lineage_mcp = MetadataChangeProposalWrapper(
        entityUrn=dataset_urn,
        aspect=lineage
    )

    # Define DataJob for the pipeline
    data_job_urn = make_data_job_urn(
        orchestrator="kafka",
        flow_id="my_etl_flow",
        job_id="my_etl_pipeline"
    )

    # Add job info with properties
    job_info = DataJobInfoClass(
        name="My ETL Pipeline",
        type="TRANSFORM",
        customProperties={
            "pipeline_name": "my_etl_pipeline",
            "scheduler": "cron: @daily"
        }
    )

    # Associate the job with input and output datasets
    job_io = DataJobInputOutputClass(
        inputDatasets=[make_dataset_urn("csv", "location_master_dimension", env="DEV")],
        outputDatasets=[make_dataset_urn("csv", "node_master_dimension", env="DEV")]
    )

    # Create MCPs for the DataJob
    job_info_mcp = MetadataChangeProposalWrapper(
        entityUrn=data_job_urn,
        aspect=job_info
    )
    job_io_mcp = MetadataChangeProposalWrapper(
        entityUrn=data_job_urn,
        aspect=job_io
    )

    # Emit lineage and DataJob metadata
    data_catalog.emit_mcp(lineage_mcp)
    data_catalog.emit_mcp(job_info_mcp)
    data_catalog.emit_mcp(job_io_mcp)
    print("Successfully updated lineage and data job info.")

if __name__ == "__main__":
    update_lineage_and_job()
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/lineage_services/data_job_service.py)


features/lineage/dataset_lineage_service.py

Python



import logging
from typing import Dict, Any
from core.platform.interface import MetadataPlatformInterface

logger = logging.getLogger(__name__)

class DatasetLineageService:
    """
    Service to manage dataset lineage.
    SRP: Its responsibility is to handle business logic related to dataset lineage.
    """

    def __init__(self, platform_handler: MetadataPlatformInterface):
        """
        Initializes the service with a platform handler.
        DIP: Depends on the MetadataPlatformInterface abstraction.
        """
        self.platform_handler = platform_handler

    def add_lineage(self, upstream_urn: str, downstream_urn: str) -> bool:
        """Adds a single lineage relationship."""
        logger.info(f"Requesting to add lineage: {upstream_urn} => {downstream_urn}")
        return self.platform_handler.add_lineage(upstream_urn, downstream_urn)

    def add_lineage_from_config(self, config: Dict[str, Any]) -> bool:
        """
        Adds lineage relationships based on a configuration dictionary.
        The config should specify upstream and downstream URNs.
        """
        lineage_info = config.get("lineage")
        if not lineage_info:
            logger.error("'lineage' key not found in the configuration.")
            return False

        downstream_urn = lineage_info.get("downstream")
        upstreams = lineage_info.get("upstreams", [])

        if not downstream_urn or not upstreams:
            logger.error("Configuration must contain 'downstream' URN and a list of 'upstreams'.")
            return False

        all_success = True
        for upstream in upstreams:
            upstream_urn = upstream.get("urn")
            if not upstream_urn:
                logger.warning("Skipping an upstream entry without a 'urn'.")
                continue

            success = self.add_lineage(upstream_urn, downstream_urn)
            if not success:
                all_success = False

        return all_success
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/lineage_services/dataset_lineage_service.py)


features/profiling/__init__.py

Python



# Profiling services (e.g., dataset stats)
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/profiling_services/__init__.py)


features/profiling/dataset_stats_service.py

Python



# Service to profile dataset statistics

class DatasetStatsService:
    def profile(self, dataset_urn):
        print(f"Profiling stats for {dataset_urn}")
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/profiling_services/dataset_stats_service.py)


features/rbac/__init__.py

Python



# RBAC Policy Service
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/rbac_services/__init__.py)


features/rbac/policy_service.py

Python



# Service for DataHub RBAC policy management

class PolicyService:
    def apply_policy(self, policy):
        print(f"Applying policy: {policy}")
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/core_library/rbac_services/policy_service.py)


sample_configs_and_templates


sample_configs_and_templates/enrichment/add_descriptions_template.yaml

YAML



# Example template to enrich with field descriptions
enrichment:
  type: description
  config:
    target_urn: urn:li:dataset:(platform,name,env)
    description: "This column contains ..."
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/sample_configs_and_templates/enrichment/add_descriptions_template.yaml)


sample_configs_and_templates/ingestion/csv_ingestion_template.yaml

YAML



# Example template for CSV ingestion
source:
  type: csv
  # Path to the source file
  path: "/Users/skalamani/Downloads/DataHub-tasks.csv"
  # Name for the dataset in the metadata platform
  dataset_name: "catalog-tasks"
  delimiter: ","

# The sink specifies where the metadata should be sent.
sink:
  type: datahub
  env: DEV
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/sample_configs_and_templates/ingestion/csv_ingestion_template.yaml)


sample_configs_and_templates/ingestion/mongo_ingestion_template.yaml

YAML



# Example template for MongoDB ingestion
source:
  type: mongodb
  # Connection URI for the MongoDB instance
  uri: "mongodb://localhost:27017"
  # The database to scan for collections
  database: "sample_db"
  # Optional: specify a list of collections to ingest.
  # If omitted, all collections in the database will be processed.
  # collections:
  #   - "collection_one"
  #   - "collection_two"

sink:
  type: datahub
  env: DEV
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/sample_configs_and_templates/ingestion/mongo_ingestion_template.yaml)


sample_configs_and_templates/lineage/dataset_lineage_template.yaml

YAML



# Example template for creating dataset lineage
lineage:
  # A list of upstream sources for the downstream dataset.
  upstreams:
    - urn: "urn:li:dataset:(urn:li:dataPlatform:csv,my-upstream-dataset,DEV)"
    - urn: "urn:li:dataset:(urn:li:dataPlatform:mongodb,sample.my-mongo-collection,DEV)"

  # The single downstream dataset that is produced from the upstreams.
  downstream: "urn:li:dataset:(urn:li:dataPlatform:hive,my_downstream_hive_table,PROD)"
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/sample_configs_and_templates/lineage/dataset_lineage_template.yaml)


Root Directory


README.md

Markdown



# DataHub Framework Toolkit

A modular and extensible framework for data ingestion, metadata management, and lineage tracking. The framework is designed to be platform-agnostic, allowing easy switching between different metadata platforms (e.g., DataHub, Databricks).

## Architecture

The framework follows a modular architecture with clear separation of concerns:

- `source_path/`: Platform-specific implementations (DataHub, Databricks, etc.)
- `core_library/`: Core framework components
  - `common/`: Shared utilities and interfaces
  - `ingestion_handlers/`: Data source ingestion handlers
  - `extraction_services/`: Schema and metadata extraction
  - `enrichment_services/`: Metadata enrichment
  - `lineage_services/`: Lineage tracking
  - `dq_services/`: Data quality
  - `profiling_services/`: Data profiling
  - `rbac_services/`: Access control
- `configs/`: Configuration files
- `sample_configs_and_templates/`: Example configurations
- `orchestration_examples/`: Example orchestration workflows

## Key Features

- Platform-agnostic design
- Modular and extensible architecture
- Type-safe interfaces
- Comprehensive error handling
- Configurable through YAML
- Support for multiple data sources
- Built-in logging and monitoring

## Installation

```bash
pip install -r requirements.txt

Usage

	1	Configure your platform and data sources in configs/
	2	Create an ingestion handler for your data source
	3	Use the framework to ingest and manage metadata
Example:
Python



from core_library.ingestion_handlers.csv_handler import CSVIngestionHandler
from core_library.common.config_manager import ConfigManager

# Initialize config manager
config_manager = ConfigManager()

# Create CSV handler
csv_handler = CSVIngestionHandler({
    "file_path": "data/example.csv",
    "delimiter": ",",
    "platform": "datahub"
}, config_manager)

# Ingest data
csv_handler.ingest("data/example.csv")

Development

	1	Install development dependencies:
Bash



pip install -r requirements-dev.txt
	2	Run tests:
Bash



pytest
	3	Format code:
Bash



black .
	4	Type checking:
Bash



mypy .

Contributing

	1	Fork the repository
	2	Create a feature branch
	3	Commit your changes
	4	Push to the branch
	5	Create a Pull Request

License

MIT License
Ingestion
Enrichment
DQ
*(Source: `sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/README.md`)*

<br>

### `framework_cli.py`
```python
import argparse
import logging
from core.common.config_manager import ConfigManager
from features.ingestion.service import IngestionService
from features.lineage.dataset_lineage_service import DatasetLineageService
from core.platform.factory import PlatformFactory

# Configure basic logging for the CLI
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def run_ingestion(config_path: str):
    """
    Initializes and runs the ingestion service based on a config file.
    """
    logger.info("Initializing framework components for ingestion...")
    config_manager = ConfigManager()

    # Load the specific ingestion config to know which platform is being used
    ingestion_config = config_manager.load_config(config_path)
    platform_name = ingestion_config.get("sink", {}).get("type", "datahub") # Default to datahub

    # Get the global config for platform connection details
    global_config = config_manager.get_global_config()
    platform_config = global_config.get(platform_name, {})

    logger.info(f"Targeting metadata platform: {platform_name}")
    platform_handler = PlatformFactory.get_instance(platform_name, platform_config)

    ingestion_service = IngestionService(config_manager, platform_handler)

    logger.info(f"Starting ingestion process for config: {config_path}")
    success = ingestion_service.start_ingestion(config_path)

    if success:
        logger.info("Ingestion process completed successfully.")
    else:
        logger.error("Ingestion process finished with errors.")

def run_add_lineage(config_path: str):
    """
    Adds dataset lineage based on a config file.
    """
    logger.info("Initializing framework components for lineage...")
    config_manager = ConfigManager()

    # Get platform config
    global_config = config_manager.get_global_config()
    # Assuming lineage sink is the default platform, this could be made more robust
    platform_name = "datahub"
    platform_config = global_config.get(platform_name, {})

    logger.info(f"Targeting metadata platform: {platform_name}")
    platform_handler = PlatformFactory.get_instance(platform_name, platform_config)

    lineage_service = DatasetLineageService(platform_handler)

    logger.info(f"Adding lineage from config: {config_path}")
    lineage_config = config_manager.load_config(config_path)
    success = lineage_service.add_lineage_from_config(lineage_config)

    if success:
        logger.info("Lineage added successfully.")
    else:
        logger.error("Failed to add lineage.")


def main():
    parser = argparse.ArgumentParser(description="Lumos Framework Toolkit CLI")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Ingestion command
    parser_ingest = subparsers.add_parser("ingest", help="Run the ingestion process")
    parser_ingest.add_argument(
        "--config-path",
        required=True,
        help="Path to the ingestion configuration YAML file."
    )

    # Lineage command
    parser_lineage = subparsers.add_parser("add-lineage", help="Add dataset lineage")
    parser_lineage.add_argument(
        "--config-path",
        required=True,
        help="Path to the lineage configuration YAML file."
    )

    args = parser.parse_args()

    if args.command == "ingest":
        run_ingestion(args.config_path)
    elif args.command == "add-lineage":
        run_add_lineage(args.config_path)
    else:
        logger.error(f"Unknown command: {args.command}")

if __name__ == "__main__":
    main()
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/framework_cli.py)


requirements.txt

datahub-client>=0.10.0
pandas>=1.5.0
pyyaml>=6.0
python-dotenv>=1.0.0
fastavro>=1.7.0
pymongo>=4.0.0
boto3>=1.26.0
pytest>=7.0.0
black>=22.0.0
mypy>=1.0.0
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/requirements.txt)


test_runner.py

Python



# Import the service and factory we just created
from features.ingestion.service import IngestionService
from features.ingestion.handlers.factory import HandlerFactory

def run_test_ingestion():
    """
    A simple function to test the IngestionService.
    """
    print(">>> Starting Ingestion Test Runner <<<")

    # This is a sample configuration, similar to your csv_ingestion_template.yaml
    # NOTE: You must update the 'filename' to a real CSV file on your machine.
    csv_job_config = {
        "source": {
            "type": "csv",
            "config": {
                "filename": "/path/to/your/file.csv",  # <--- IMPORTANT: CHANGE THIS PATH
                "platform": "csv",
                "env": "DEV",
            },
        },
        "sink": {
            "type": "datahub-rest",
            "config": {
                "server": "http://localhost:8080",
            },
        },
    }

    # 1. Create an instance of our factory
    factory = HandlerFactory()

    # 2. Create an instance of the IngestionService, giving it the factory
    ingestion_service = IngestionService(handler_factory=factory)

    # 3. Run the ingestion job with our sample configuration
    ingestion_service.ingest(csv_job_config)

    print("\n>>> Test Runner Finished <<<")


if __name__ == "__main__":
    run_test_ingestion()
(Source: sushant-runs/project-lumos/sushant-runs-project-lumos-722662ef0176e51c2de17a82200d72de41afcc73/test_runner.py)
